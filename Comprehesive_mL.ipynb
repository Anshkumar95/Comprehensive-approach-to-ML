{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:46.444334Z",
     "start_time": "2024-08-19T10:59:45.843606Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from datetime import datetime \n",
    "from tqdm import tqdm\n",
    "\n",
    "import openpyxl\n",
    "import os\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, BorderlineSMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, matthews_corrcoef\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, RFE\n",
    "\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier, LabelSpreading, LabelPropagation\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:46.447041Z",
     "start_time": "2024-08-19T10:59:46.445271Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "\n",
    "# def create_requirements_file(output_file='requirements.txt'):\n",
    "#     # Get the list of all installed packages\n",
    "#     installed_packages = pkg_resources.working_set\n",
    "\n",
    "#     # Write the package names and versions to the output file\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         for package in installed_packages:\n",
    "#             f.write(f\"{package.project_name}=={package.version}\\n\")\n",
    "\n",
    "#     print(f\"All dependencies have been written to {output_file}\")\n",
    "\n",
    "# # Call the function to create the requirements file\n",
    "# create_requirements_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:51.525475Z",
     "start_time": "2024-08-19T10:59:51.516330Z"
    }
   },
   "outputs": [],
   "source": [
    "def separate_columns(df):\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['object', 'category'] or df[col].nunique() <= 10:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            numerical_cols.append(col)\n",
    "    \n",
    "    return categorical_cols, numerical_cols\n",
    "\n",
    "categorical_cols, numerical_cols = separate_columns(df)#### Insert df of choice\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numerical columns:\", numerical_cols)\n",
    "\n",
    "\n",
    "num_col = numerical_cols\n",
    "\n",
    "cat_col = categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:52.082010Z",
     "start_time": "2024-08-19T10:59:52.074856Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_value_counts(df):\n",
    "    categorical_feat = categorical_cols\n",
    "    \n",
    "    for col in categorical_feat:\n",
    "        print(f\"Value counts for column: {col}\")\n",
    "        print(df[col].value_counts())\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "display_value_counts(df) #### Insert df of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Imputation strategies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:53.910108Z",
     "start_time": "2024-08-19T10:59:53.907052Z"
    }
   },
   "outputs": [],
   "source": [
    "Imputation_data_dictionary = {\n",
    "    \"Median\": [\"List the features you want to impute using median strategy\"\n",
    "    ],\n",
    "    \"Frequent\": ['List the features you want to impute using mode strategy'\n",
    "    ],\n",
    "    \"MICE\": [\"List the features you want to impute using MICE strategy\"\n",
    "    ],\n",
    "    \"KNN\": ['List the features you want to impute using KNN strategy'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:55.323992Z",
     "start_time": "2024-08-19T10:59:55.321240Z"
    }
   },
   "outputs": [],
   "source": [
    "Knn_Imputation_dictionary = {\n",
    "    \"features_for_imputation\": [\"independent features for KNN\"\n",
    "    ],\n",
    "    \"columns_to_impute\": [ 'the column that need imputaion using knn'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:56.425421Z",
     "start_time": "2024-08-19T10:59:56.420722Z"
    }
   },
   "outputs": [],
   "source": [
    "def null_values_summary(df):\n",
    "    # Create a DataFrame to store the results\n",
    "    summary = pd.DataFrame(columns=['Column', 'Null Count', 'Null Percentage'])\n",
    "    \n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_percentage = (null_count / len(df)) * 100\n",
    "        fill = df[col].count()\n",
    "        distinct_count = df[col].nunique()\n",
    "        if distinct_count <= 10:\n",
    "            unique = df[col].unique()\n",
    "        else:\n",
    "            unique = \"N/A\"\n",
    "        summary = pd.concat([summary, pd.DataFrame({'Column': [col], 'Null Count': [null_count], 'Null Percentage': [null_percentage],\n",
    "                                                    'Fill count': [fill],\n",
    "                                                    'Distinct value count': [distinct_count], 'Unique values': [unique], 'Type': [None]\n",
    "                                                    })], ignore_index=True)\n",
    "    \n",
    "    # Plotting the results\n",
    "    summary.set_index('Column', inplace=True)\n",
    "    summary['Null Percentage'].plot(kind='barh', figsize=(10, 8), color='skyblue')\n",
    "    plt.xlabel('Percentage of Null Values')\n",
    "    plt.title('Percentage of Null Values by Column')\n",
    "    plt.show()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:58.720996Z",
     "start_time": "2024-08-19T10:59:58.430562Z"
    }
   },
   "outputs": [],
   "source": [
    "null_eda_df = null_values_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T10:59:59.449229Z",
     "start_time": "2024-08-19T10:59:59.394710Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def impute_df(df, Imputation_data_dictionary, Knn_Imputation_dictionary):\n",
    "    \"\"\"\n",
    "    Impute missing values in the DataFrame based on the specified rules.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        Imputation_data_dictionary (dict): Dictionary with data source categories as keys and list of column names as values.\n",
    "        Knn_Imputation_dictionary (dict): Dictionary with KNN imputation details.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    # Extract features for imputation and columns to impute from the dictionary\n",
    "    features_for_imputation = Knn_Imputation_dictionary[\"features_for_imputation\"]\n",
    "    columns_to_impute = Knn_Imputation_dictionary[\"columns_to_impute\"]\n",
    "    \n",
    "    # Create the KNN imputer\n",
    "    knn_imputer = KNNImputer()\n",
    "    # \n",
    "    # Create the median imputer\n",
    "    median_imputer = SimpleImputer(strategy='median')\n",
    "    # Initialize the mode imputer\n",
    "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Create the Iterative imputer with RF regressor estimator\n",
    "    rf_estimator = RandomForestRegressor()\n",
    "    iterative_imputer = IterativeImputer(estimator=rf_estimator)\n",
    "    \n",
    "    # Impute based on the strategy in the dictionary\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():  # Check if the column has any missing values\n",
    "            print(f\"Column '{col}' has missing values.\")\n",
    "            if col in Imputation_data_dictionary['Median']:\n",
    "                print(f\"Imputing {col} using Median strategy.\")\n",
    "                df[col] = median_imputer.fit_transform(df[[col]])\n",
    "            elif col in Imputation_data_dictionary['KNN_Imputer']:\n",
    "                if col in columns_to_impute:\n",
    "                    print(f\"Imputing {col} using KNN strategy with features: {features_for_imputation}\")\n",
    "                    # Create a DataFrame with the selected features and the column to impute\n",
    "                    df_to_impute = df[features_for_imputation + [col]]\n",
    "                    # Perform the KNN imputation\n",
    "                    imputed_data = knn_imputer.fit_transform(df_to_impute)\n",
    "                    # Update the column with the imputed values\n",
    "                    df[col] = imputed_data[:, -1]\n",
    "            elif col in Imputation_data_dictionary['Frequent']:\n",
    "                print(f\"Imputing {col} using Mode Imputer.\")\n",
    "                df[[col]] = mode_imputer.fit_transform(df[[col]])\n",
    "            elif col in Imputation_data_dictionary['MICE']:\n",
    "                print(f\"Imputing {col} using MICE Imputer.\")\n",
    "                df[[col]] = iterative_imputer.fit_transform(df[[col]])\n",
    "        else:\n",
    "            print(f\"Column '{col}' has no missing values, no imputation needed.\")\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df_pre_imp = df.copy()  #### Insert df of choice\n",
    "imputed_df = impute_df(df_pre_imp, Imputation_data_dictionary, Knn_Imputation_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:00:10.133519Z",
     "start_time": "2024-08-19T11:00:09.866457Z"
    }
   },
   "outputs": [],
   "source": [
    "null_eda_df_1 = null_values_summary(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:00:10.495959Z",
     "start_time": "2024-08-19T11:00:10.451824Z"
    }
   },
   "outputs": [],
   "source": [
    "imputed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:00:10.762772Z",
     "start_time": "2024-08-19T11:00:10.713984Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:00:26.607444Z",
     "start_time": "2024-08-19T11:00:26.605442Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive pipeline to help select best model and transformation along with outliering technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFE, SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import zscore, iqr, skew\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to handle outliers on a per-column basis\n",
    "def handle_outliers_columnwise(X, method='zscore', threshold=3):\n",
    "    X_outliers_handled = X.copy()\n",
    "    for column in X.columns:\n",
    "        if method == 'zscore':\n",
    "            X_outliers_handled = X_outliers_handled[(np.abs(zscore(X_outliers_handled[column])) < threshold)]\n",
    "        elif method == 'iqr':\n",
    "            lower_bound = X[column].quantile(0.25) - (1.5 * iqr(X[column]))\n",
    "            upper_bound = X[column].quantile(0.75) + (1.5 * iqr(X[column]))\n",
    "            X_outliers_handled = X_outliers_handled[(X_outliers_handled[column] >= lower_bound) & (X_outliers_handled[column] <= upper_bound)]\n",
    "        elif method == 'isolation_forest':\n",
    "            iso = IsolationForest(contamination=0.1, random_state=42)\n",
    "            yhat = iso.fit_predict(X_outliers_handled[[column]])\n",
    "            X_outliers_handled = X_outliers_handled[yhat != -1]\n",
    "    return X_outliers_handled\n",
    "\n",
    "# Function to determine the best transformation for each column based on skewness\n",
    "def get_best_transformations(X):\n",
    "    transformations = []\n",
    "    best_transformations = {}\n",
    "    for i, column in enumerate(X.columns):\n",
    "        col_skew = skew(X[column])\n",
    "        if col_skew > 1:\n",
    "            if (X[column] > 0).all():\n",
    "                transformations.append((f'{column}_boxcox', PowerTransformer(method='box-cox'), [i]))\n",
    "                best_transformations[column] = 'Box-Cox'\n",
    "            else:\n",
    "                transformations.append((f'{column}_yeojohnson', PowerTransformer(method='yeo-johnson'), [i]))\n",
    "                best_transformations[column] = 'Yeo-Johnson'\n",
    "        elif 0.5 < col_skew <= 1:\n",
    "            transformations.append((f'{column}_sqrt', FunctionTransformer(np.sqrt), [i]))\n",
    "            best_transformations[column] = 'Square Root'\n",
    "        else:\n",
    "            transformations.append((f'{column}_none', FunctionTransformer(None), [i]))\n",
    "            best_transformations[column] = 'None'\n",
    "    return transformations, best_transformations\n",
    "\n",
    "# Define models to be used\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Define outlier handling techniques\n",
    "outlier_methods = ['zscore', 'iqr', 'isolation_forest']\n",
    "\n",
    "# Define feature selection methods\n",
    "feature_selectors = {\n",
    "    'RFE': RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=10),\n",
    "    'SelectKBest': SelectKBest(score_func=f_classif, k=10),\n",
    "}\n",
    "\n",
    "# Store the best results\n",
    "best_results = {\n",
    "    'model': None,\n",
    "    'f1_score': 0,\n",
    "    'outlier_method': None,\n",
    "    'transformation': None,\n",
    "    'features': None,\n",
    "    'best_transformations': None,  # To store the best transformations per column\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "X = df_test.drop(columns=['Target', 'party_id','rank_1'])\n",
    "y = df_test['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get column-specific transformations based on the training data\n",
    "transformations, best_transformations = get_best_transformations(X_train)\n",
    "\n",
    "# Create the column transformer using column indices\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=transformations,\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Total number of combinations\n",
    "total_combinations = len(outlier_methods) * len(feature_selectors) * len(models)\n",
    "progress_bar = None\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=total_combinations, desc=\"Processing Combinations\")\n",
    "\n",
    "# Iterate through all combinations of outlier methods, feature selection, and models\n",
    "for outlier_method in outlier_methods:\n",
    "    # Handle outliers\n",
    "    X_train_outliers_handled = handle_outliers_columnwise(X_train.copy(), method=outlier_method)\n",
    "    y_train_outliers_handled = y.loc[X_train_outliers_handled.index]\n",
    "\n",
    "    for feature_selector_name, feature_selector in feature_selectors.items():\n",
    "        for model_name, model in models.items():\n",
    "            # Create a pipeline\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
    "                ('transformer', column_transformer),  # Apply column-specific transformations\n",
    "                ('scaler', StandardScaler()),  # Standardize features\n",
    "                ('feature_selection', feature_selector),  # Feature selection\n",
    "                ('model', model)  # Model training\n",
    "            ])\n",
    "            \n",
    "            # Train and evaluate using cross-validation\n",
    "            f1_scores = cross_val_score(pipeline, X_train_outliers_handled, y_train_outliers_handled, cv=3, scoring='f1_macro')\n",
    "            mean_f1_score = np.mean(f1_scores)\n",
    "            \n",
    "            # Check if this is the best model so far\n",
    "            if mean_f1_score > best_results['f1_score']:\n",
    "                best_results['model'] = model_name\n",
    "                best_results['f1_score'] = mean_f1_score\n",
    "                best_results['outlier_method'] = outlier_method\n",
    "                best_results['transformation'] = 'Column-Specific'\n",
    "                best_results['features'] = feature_selector_name\n",
    "                best_results['best_transformations'] = best_transformations\n",
    "            \n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Print the best results\n",
    "print(f\"Best Model: {best_results['model']}\")\n",
    "print(f\"Best F1 Score: {best_results['f1_score']}\")\n",
    "print(f\"Best Outlier Handling Method: {best_results['outlier_method']}\")\n",
    "print(f\"Best Transformation: {best_results['transformation']}\")\n",
    "print(f\"Best Feature Selection Method: {best_results['features']}\")\n",
    "\n",
    "# Print the best transformations for each column\n",
    "print(\"\\nBest Transformations for Each Column:\")\n",
    "for column, transformation in best_results['best_transformations'].items():\n",
    "    print(f\"Column: {column}, Transformation: {transformation}\")\n",
    "\n",
    "# Final model training on the entire training set and evaluation on the test set\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('transformer', column_transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', feature_selectors[best_results['features']]),\n",
    "    ('model', models[best_results['model']])\n",
    "])\n",
    "\n",
    "# Handle outliers in the full training set\n",
    "X_train_outliers_handled = handle_outliers_columnwise(X_train.copy(), method=best_results['outlier_method'])\n",
    "y_train_outliers_handled = y.loc[X_train_outliers_handled.index]\n",
    "\n",
    "# Fit the final model\n",
    "final_pipeline.fit(X_train_outliers_handled, y_train_outliers_handled)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_test = final_pipeline.predict(X_test)\n",
    "print(\"\\nFinal Model Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking outlier count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def count_outliers(df):\n",
    "    outlier_counts = {}\n",
    "   \n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[col].dtype in ['int64', 'float64'] and col != 'party_id' and df[col].nunique() > 11 and col in Imputation_data_dictionary['Median']:\n",
    "            # Outlier treatment using Median Absolute Deviation (MAD)\n",
    "            median = df[col].median()\n",
    "            mad = stats.median_abs_deviation(df[col])\n",
    "            lower_bound = median - 3 * mad\n",
    "            upper_bound = median + 3 * mad\n",
    "           \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_counts[col] = outliers.shape[0]\n",
    "   \n",
    "    return outlier_counts\n",
    " \n",
    "def plot_outlier_counts(outlier_counts):\n",
    "    columns = list(outlier_counts.keys())\n",
    "    counts = list(outlier_counts.values())\n",
    "   \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(columns, counts, color='skyblue')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Number of Outliers')\n",
    "    plt.title('Count of Outliers in Each Column')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    " \n",
    "# # Example usage\n",
    "# data = {\n",
    "#     'A': [1, 2, 2, 3, 4, 100, 6, 7, 8, 9],\n",
    "#     'B': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "#     'C': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    " \n",
    "df_test_2 = df_xfr.copy()\n",
    " \n",
    "outlier_counts = count_outliers(df_test_2)\n",
    "print(outlier_counts)\n",
    "plot_outlier_counts(outlier_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using isolation forest for outlier removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:12:48.718297Z",
     "start_time": "2024-08-19T11:12:48.703084Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def isolation_forest_outlier_removal(df, initial_contamination=0.1, max_attempts=5, contamination_adjustment=0.02):\n",
    "    \"\"\"\n",
    "    This function removes outliers based on the Isolation Forest method.\n",
    "    The function will iteratively adjust the contamination level if the skewness does not reduce after outlier removal.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64'] and col != 'party_id' and col in Imputation_data_dictionary['Median'] and col not in ['health_envrnmnt_index', 'comp_ptnt_cnt', 'ethnic_hispanic_pct', 'org_hcp_cnt', 'race_black_pct','popltn_wo_health_insrnc']:\n",
    "            \n",
    "            attempts = 0\n",
    "            skew_before = df[col].skew()\n",
    "            skew_after = skew_before\n",
    "            contamination = initial_contamination\n",
    "            \n",
    "            while abs(skew_after) >= abs(skew_before) and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                print(f\"Attempt {attempts}: Adjusting contamination level for column '{col}' (current contamination={contamination})\")\n",
    "                print(f\"Skewness of {col} before outlier removal: {skew_before:.4f}\")\n",
    "                \n",
    "                # Apply Isolation Forest\n",
    "                iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "                df['outlier_flag'] = iso_forest.fit_predict(df[[col]])\n",
    "                \n",
    "                # Identify outliers (where flag is -1)\n",
    "                outliers = df['outlier_flag'] == -1\n",
    "                \n",
    "                # Replace outliers with the median of the column\n",
    "                median = df[col].median()\n",
    "                df.loc[outliers, col] = median\n",
    "                \n",
    "                # Recalculate skewness after outlier removal\n",
    "                skew_after = df[col].skew()\n",
    "                print(f\"Skewness of {col} after outlier removal: {skew_after:.4f}\")\n",
    "                \n",
    "                if abs(skew_after) >= abs(skew_before):\n",
    "                    # Adjust the contamination level and retry\n",
    "                    contamination -= contamination_adjustment\n",
    "                    contamination = max(0.01, contamination)  # Ensure contamination stays positive\n",
    "                    print(f\"Skewness did not reduce. Adjusting contamination level to {contamination}.\")\n",
    "                \n",
    "                # Remove the outlier flag column\n",
    "                df.drop(columns=['outlier_flag'], inplace=True)\n",
    "            \n",
    "            if abs(skew_after) >= abs(skew_before):\n",
    "                print(f\"Warning: Even after {attempts} attempts, skewness for column '{col}' did not reduce significantly. Final skewness: {skew_after:.4f}\")\n",
    "            else:\n",
    "                print(f\"Success: Skewness reduced for column '{col}' after {attempts} attempts. Final skewness: {skew_after:.4f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df_test_1 = df_xfr.copy()\n",
    "df_transformed_engagers = isolation_forest_outlier_removal(df_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing best transformation per column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def robust_transformation(df, skew_threshold=0.5):\n",
    "    \"\"\"\n",
    "    This function applies the appropriate transformation to each numeric column in the dataframe\n",
    "    to make the distribution as close to normal as possible. It selects the transformation based on the skewness\n",
    "    and ensures that the skewness is reduced after transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the data to be transformed.\n",
    "    - skew_threshold: float, the threshold above which transformations are considered for reducing skewness.\n",
    "\n",
    "    Returns:\n",
    "    - transformed_df: DataFrame, the transformed data.\n",
    "    - transformation_log: dict, log of transformations applied to each column.\n",
    "    \"\"\"\n",
    "    transformation_log = {}\n",
    "\n",
    "    def log_transform(col):\n",
    "        return np.log1p(col)\n",
    "\n",
    "    def sqrt_transform(col):\n",
    "        return np.sqrt(col)\n",
    "\n",
    "    def inverse_transform(col):\n",
    "        return 1 / (col + 1e-6)  # Small epsilon to avoid division by zero\n",
    "\n",
    "    def boxcox_transform(col):\n",
    "        # Box-Cox can only be applied to positive values\n",
    "        transformed_col, _ = boxcox(col + 1e-6)  # Adding a small epsilon to avoid zero values\n",
    "        return transformed_col\n",
    "\n",
    "    def square_transform(col):\n",
    "        return np.power(col, 2)\n",
    "\n",
    "    def cube_transform(col):\n",
    "        return np.power(col, 3)\n",
    "\n",
    "    def apply_transformation(col, skewness):\n",
    "        transformation_methods = []\n",
    "\n",
    "        if skewness > 2:\n",
    "            if (col > 0).all():\n",
    "                transformation_methods.append(('Box-Cox', boxcox_transform))\n",
    "            transformation_methods.append(('Log', log_transform))\n",
    "        if skewness > 1:\n",
    "            transformation_methods.append(('Square Root', sqrt_transform))\n",
    "        if skewness < -1:\n",
    "            transformation_methods.append(('Inverse', inverse_transform))\n",
    "        if skewness < -0.5:\n",
    "            transformation_methods.append(('Cube', cube_transform))\n",
    "        if -0.5 < skewness < 0:\n",
    "            transformation_methods.append(('Square', square_transform))\n",
    "\n",
    "        return transformation_methods\n",
    "\n",
    "    transformed_df = df.copy()\n",
    "\n",
    "    for col in transformed_df.columns:\n",
    "        if transformed_df[col].dtype in ['int64', 'float64'] and col != 'party_id' and col in Imputation_data_dictionary['Median'] and col not in ['health_envrnmnt_index', 'comp_ptnt_cnt', 'ethnic_hispanic_pct', 'org_hcp_cnt', 'race_black_pct','popltn_wo_health_insrnc']:\n",
    "            skew_before = transformed_df[col].skew()\n",
    "            print(f\"Initial skewness for column '{col}': {skew_before:.4f}\")\n",
    "\n",
    "            if abs(skew_before) > skew_threshold:\n",
    "                transformation_methods = apply_transformation(transformed_df[col], skew_before)\n",
    "                \n",
    "                for method_name, transform_func in transformation_methods:\n",
    "                    transformed_col = transform_func(transformed_df[col])\n",
    "                    skew_after = pd.Series(transformed_col).skew()\n",
    "                    print(f\"Attempting {method_name} transformation on column '{col}'...\")\n",
    "                    print(f\"Skewness after {method_name}: {skew_after:.4f}\")\n",
    "\n",
    "                    if abs(skew_after) < abs(skew_before):\n",
    "                        transformed_df[col] = transformed_col\n",
    "                        transformation_log[col] = method_name\n",
    "                        print(f\"Success: {method_name} transformation reduced skewness for column '{col}' to {skew_after:.4f}\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"{method_name} transformation did not reduce skewness for column '{col}'.\")\n",
    "                else:\n",
    "                    print(f\"Warning: None of the transformations reduced skewness for column '{col}'.\")\n",
    "                    transformation_log[col] = 'None'\n",
    "            else:\n",
    "                print(f\"No transformation applied to column '{col}' (skewness: {skew_before:.4f})\")\n",
    "                transformation_log[col] = 'None'\n",
    "\n",
    "    return transformed_df, transformation_log\n",
    "\n",
    "# Example usage\n",
    "df_test_3 = df_transformed_engagers.copy()  # Replace df_xfr with your dataframe\n",
    "df_transformed_engagers_xfr, transformation_log = robust_transformation(df_test_3)\n",
    "\n",
    "# View the log of transformations applied\n",
    "print(\"\\nTransformation Log:\")\n",
    "for col, trans in transformation_log.items():\n",
    "    print(f\"{col}: {trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick feature selection code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif, RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "def feature_selection(df, target_column, variance_threshold=0.1, k_range=range(6, 31)):\n",
    "    \"\"\"\n",
    "    Perform feature selection with variance threshold, SelectKBest, and RFE.\n",
    "    The best value of k in SelectKBest is chosen based on the highest F1-weighted score.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, input data\n",
    "    - target_column: str, name of the target column\n",
    "    - variance_threshold: float, threshold for variance threshold selection\n",
    "    - k_range: range, range of k values to try for SelectKBest\n",
    "\n",
    "    Returns:\n",
    "    - df_selected: DataFrame, data with selected features\n",
    "    - selected_features: list, list of selected features\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column,'party_id'])\n",
    "    y = df[target_column]\n",
    "   \n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "   \n",
    "    # Encode categorical columns\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "   \n",
    "    # 1. Remove features with low variance\n",
    "    selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    X_var = selector.fit_transform(X)\n",
    "    selected_features_var = X.columns[selector.get_support(indices=True)]\n",
    "   \n",
    "    # 2. Select K best features based on mutual information\n",
    "    best_k = None\n",
    "    best_score = -np.inf\n",
    "    f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "    for k in k_range:\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "        X_kbest = selector.fit_transform(X_var, y)\n",
    "        scores = cross_val_score(XGBClassifier(random_state=42, eval_metric='mlogloss'), X_kbest, y, cv=3, scoring=f1_scorer)\n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"F1-weighted score for k={k}: {mean_score:.4f}\")\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "            selected_features_kbest = selected_features_var[selector.get_support(indices=True)]\n",
    "    \n",
    "    print(f\"\\nBest k value based on F1-weighted score: {best_k}\")\n",
    "   \n",
    "    # 3. Recursive Feature Elimination (RFE) with XGBoost\n",
    "    model = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "    selector = RFE(model, n_features_to_select=best_k)\n",
    "    X_rfe = selector.fit_transform(X[selected_features_kbest], y)\n",
    "    selected_features_rfe = selected_features_kbest[selector.get_support(indices=True)]\n",
    "   \n",
    "    # Combine selected features\n",
    "    selected_features = selected_features_rfe\n",
    "   \n",
    "    # Create a DataFrame with the selected features\n",
    "    df_selected = df[selected_features]\n",
    "    df_selected[target_column] = y\n",
    "   \n",
    "    return df_selected, selected_features\n",
    " \n",
    "# Example usage\n",
    "target_column = 'Target'\n",
    "df_selected, selected_features = feature_selection(df_transformed_engagers_xfr, target_column)\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT and Sampling function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:13:19.625901Z",
     "start_time": "2024-08-19T11:13:19.619477Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def prepare_data_with_sampling(X, y, test_size=0.2, random_state=42, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    This function splits the data into train and test sets, applies smote and returns the resampled datasets.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame or ndarray): Feature matrix.\n",
    "    y (Series or ndarray): Target vector.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    sampling_strategy (str, float, dict): Sampling strategy for SMOTE and RandomUnderSampler.\n",
    "\n",
    "    Returns:\n",
    "    X_train (ndarray): Training features before resampling.\n",
    "    y_train (ndarray): Training labels before resampling.\n",
    "    X_test (ndarray): Test features.\n",
    "    y_test (ndarray): Test labels.\n",
    "    X_resampled (ndarray): Resampled training features after undersampling and oversampling.\n",
    "    y_resampled (ndarray): Resampled training labels after undersampling and oversampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    \n",
    "    # Apply SMOTE to balance the training data after undersampling\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_resampled, y_resampled\n",
    "\n",
    "# Example usage\n",
    "# X, y = your_data  # Replace with your actual data\n",
    "# X_train, y_train, X_test, y_test, X_resampled, y_resampled = prepare_data_with_sampling(X, y)\n",
    "\n",
    "def plot_feature_importance(importance, features, model_name):\n",
    "    # Ensure the features list matches the number of importances\n",
    "    print(\"Number of features\",len(features))\n",
    "    print(\"Number of importance\",len(importance))\n",
    "    if len(features) != len(importance):\n",
    "        raise ValueError(\"The length of features does not match the number of importances\")\n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "\n",
    "    # Rearrange feature names so they match the sorted feature importances\n",
    "    names = [features[i] for i in indices]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(5, 15))\n",
    "\n",
    "    # Create plot title\n",
    "    plt.title(f\"Feature Importance for {model_name}\")\n",
    "    plt.barh(range(len(importance)), importance[indices])\n",
    "\n",
    "    # Add feature names as x-axis labels\n",
    "    plt.yticks(range(len(importance)), names)\n",
    "    \n",
    "    # Invert y-axis to have the most important features on top\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test and train with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:13:20.826890Z",
     "start_time": "2024-08-19T11:13:20.661583Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df_transformed_engagers is already defined\n",
    "\n",
    "X = df.drop(columns=['Target'])\n",
    "y = df['Target']\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_train_smote, y_train_smote = prepare_data_with_sampling(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store the trained models\n",
    "Random_forest_model = None\n",
    "Adaboost_model = None\n",
    "GB_model = None\n",
    "XGB_model = None\n",
    "\n",
    "\n",
    "def train_and_evaluate(models, param_grids, X_train, y_train, X_test, y_test, feature_names, class_weights=None):\n",
    "    global Random_forest_model, Adaboost_model, GB_model, XGB_model\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training and tuning {name}...\")\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], verbose=1, n_jobs=-1, return_train_score=True, cv=3, scoring='f1')\n",
    "        \n",
    "        # If class weights are provided, use them during training\n",
    "        if name == 'Random Forest':\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            Random_forest_model = grid_search.best_estimator_\n",
    "        elif name == 'Gradient Boosting':\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            GB_model = grid_search.best_estimator_\n",
    "        elif name == 'Extreme Gradient Boost':\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            XGB_model = grid_search.best_estimator_\n",
    "        else:\n",
    "            if class_weights and hasattr(model, 'class_weight'):\n",
    "                grid_search.fit(X_train, y_train, class_weight=class_weights)\n",
    "            else:\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                Adaboost_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = grid_search.predict(X_train)\n",
    "        y_pred_test = grid_search.predict(X_test)\n",
    "                \n",
    "        # Print classification reports\n",
    "        print(f\"\\nClassification Report for Training Set ({name}):\")\n",
    "        print(classification_report(y_train, y_pred_train))\n",
    "        print(\"\\n\")\n",
    "        train_cnf_matrix = confusion_matrix(y_train, y_pred_train)\n",
    "        disp_train = ConfusionMatrixDisplay(confusion_matrix=train_cnf_matrix)\n",
    "        disp_train.plot(cmap='viridis', values_format='d')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nClassification Report for Test Set ({name}):\")\n",
    "        print(classification_report(y_test, y_pred_test))\n",
    "        print(\"\\n\")\n",
    "        test_cnf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "        disp_test = ConfusionMatrixDisplay(confusion_matrix=test_cnf_matrix)\n",
    "        disp_test.plot(cmap='inferno', values_format='d')\n",
    "        plt.show()\n",
    "            \n",
    "        # Plot feature importance if the model has the attribute\n",
    "        if hasattr(grid_search.best_estimator_, 'feature_importances_'):\n",
    "            plot_feature_importance(grid_search.best_estimator_.feature_importances_, feature_names, name)\n",
    "        elif hasattr(grid_search.best_estimator_, 'coef_'):\n",
    "            plot_feature_importance(np.abs(grid_search.best_estimator_.coef_[0]), feature_names, name)\n",
    "            \n",
    "        # Evaluate the model using cross-validation\n",
    "        cv_scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=3, scoring='f1')\n",
    "        print(f\"Cross-validation scores for {name}: {cv_scores}\")\n",
    "        print(f\"Mean cross-validation score for {name}: {np.mean(cv_scores)}\")\n",
    "        \n",
    "        # Print best parameters\n",
    "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Define the base classifiers and parameter grids\n",
    "models = {\n",
    "    'Extreme Gradient Boost': XGBClassifier(random_state=12,eval_metric='mlogloss'),\n",
    "}\n",
    "\n",
    "# models = {\n",
    "#     'Random Forest': RandomForestClassifier(random_state=12, class_weight='balanced'),   \n",
    "#     'Gradient Boosting': GradientBoostingClassifier(random_state=12),\n",
    "#     'Extreme Gradient Boost': XGBClassifier(random_state=12,eval_metric='mlogloss')\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grids for each model\n",
    "param_grids = {\n",
    "    'Extreme Gradient Boost': {\n",
    "        'n_estimators': [ ],\n",
    "        'learning_rate': [  ],\n",
    "        'max_depth': [ ],\n",
    "        'min_child_weight': [ ],\n",
    "        'gamma': [ ],\n",
    "        'lambda': [ ],\n",
    "        'alpha': [ ]\n",
    "    }\n",
    "}\n",
    "feature_names = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Smote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:13:31.859451Z",
     "start_time": "2024-08-19T11:13:29.867105Z"
    }
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(models, param_grids, X_train_smote, y_train_smote, X_test, y_test,feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:14:08.052057Z",
     "start_time": "2024-08-19T11:14:08.047865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(alpha=5, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=10,\n",
       "              learning_rate=0.005, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=15,\n",
       "              max_leaves=None, min_child_weight=10, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n",
       "              n_jobs=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(alpha=5, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=10,\n",
       "              learning_rate=0.005, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=15,\n",
       "              max_leaves=None, min_child_weight=10, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n",
       "              n_jobs=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(alpha=5, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=10,\n",
       "              learning_rate=0.005, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=15,\n",
       "              max_leaves=None, min_child_weight=10, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=300,\n",
       "              n_jobs=None, ...)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T11:14:11.410102Z",
     "start_time": "2024-08-19T11:14:10.334299Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming xgb_model_tuned.best_estimator_ is the trained model and X_train_smote is the training data\n",
    "explainer = shap.TreeExplainer(XGB_model)\n",
    "SHAP_values = explainer.shap_values(X_train_smote)\n",
    "shap.initjs()\n",
    "\n",
    "# Define the target classes\n",
    "target_classes = ['list the target classes as required '] #### change as per preferance \n",
    "\n",
    "# Plot SHAP values for each target class\n",
    "for cnt, target_class in enumerate(target_classes):\n",
    "    plt.title(\n",
    "        f\"==============================================\\nSHAP Values for {target_class}\\n==============================================\")\n",
    "    shap.summary_plot(SHAP_values[:, :, cnt], X_train_smote, plot_type='dot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Shap feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 2\n",
    "explainer = shap.TreeExplainer(XGB_model)  # Use the best estimator from GridSearchCV\n",
    "SHAP_values = explainer.shap_values(X_train_smote)\n",
    "\n",
    "# Create DataFrame for absolute mean SHAP values\n",
    "df_absmeanshap = pd.DataFrame(list(zip(X_train.columns, np.abs(SHAP_values[:,:,i]).mean(axis=0))), columns=['features', 'absmeanshap'])\n",
    "\n",
    "# Create DataFrame for mean SHAP values\n",
    "df_meanshap = pd.DataFrame(list(zip(X_train.columns, np.abs(SHAP_values[:,:,i]).mean(axis=0))), columns=['features', 'meanshap'])\n",
    "\n",
    "# Merge the two DataFrames on 'features'\n",
    "df_f = pd.merge(df_absmeanshap, df_meanshap, on='features', how='left')\n",
    "\n",
    "# Sort by 'absmeanshap' in descending order and select top 20 rows\n",
    "sorted_df = df_f.sort_values(by='absmeanshap', ascending=False).head(20)\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def evaluate_model(self, model, X_train_subset, X_test_subset):\n",
    "        model.fit(X_train_subset, self.y_train)\n",
    "        y_pred = model.predict(X_test_subset)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        return accuracy, f1\n",
    "    \n",
    "\n",
    "    def filter_method(self, k=25):\n",
    "        selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        X_new = selector.fit_transform(self.X_train_scaled, self.y_train)\n",
    "        selected_features = selector.get_support(indices=True)\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        accuracy, f1 = self.evaluate_model(model, self.X_train_scaled[:, selected_features], self.X_test_scaled[:, selected_features])\n",
    "        print(f\"Filter Method - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "        return selected_features\n",
    "\n",
    "    def wrapper_rfe(self):\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        rfe = RFE(model, n_features_to_select=25)  # Adjust the number as needed\n",
    "        fit = rfe.fit(self.X_train_scaled, self.y_train)\n",
    "        rfe_features = np.where(fit.support_)[0]\n",
    "        accuracy, f1 = self.evaluate_model(model, self.X_train_scaled[:, rfe_features], self.X_test_scaled[:, rfe_features])\n",
    "        print(f\"RFE Method - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "        return rfe_features\n",
    "    \n",
    "    def exhaustive_feature_selection(self, model, min_features=4, max_features=20):\n",
    "        best_score = 0\n",
    "        best_features = None\n",
    "        n_features = self.X_train_scaled.shape[1]\n",
    "        \n",
    "        total_combinations = sum([len(list(combinations(range(n_features), k))) for k in range(1, max_features + 1)])\n",
    "                \n",
    "        with tqdm(total=total_combinations, desc=\"Feature Selection Progress\") as pbar:\n",
    "            for k in range(1, max_features + 1):\n",
    "                for subset in combinations(range(n_features), k):\n",
    "                    X_train_subset = self.X_train_scaled[:, subset]\n",
    "                    X_test_subset = self.X_test_scaled[:, subset]\n",
    "                    accuracy, f1 = self.evaluate_model(clone(model), X_train_subset, X_test_subset)\n",
    "                    if f1 > best_score:\n",
    "                        best_score = f1\n",
    "                        best_features = subset\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "        print(f\"Exhaustive Feature Selection - Best f1: {best_score:.4f}\")\n",
    "        return best_features\n",
    "\n",
    "    def wrapper_sfs(self):\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        sfs = SFS(model, \n",
    "                      k_features='14', \n",
    "                  floating=True, \n",
    "                  scoring='f1_weighted',\n",
    "                  cv=5,              \n",
    "                  n_jobs=-1,\n",
    "                  pre_dispatch='2*n_jobs',\n",
    "                  verbose=0)  # Turn off verbose in SFS and use tqdm instead\n",
    "        \n",
    "            # Adding a progress bar\n",
    "        n_features = self.X_train_scaled.shape[1]\n",
    "        with tqdm(total=n_features) as pbar:\n",
    "            for i in range(1, n_features + 1):\n",
    "                sfs.k_features = i\n",
    "                sfs = sfs.fit(self.X_train_scaled, self.y_train)\n",
    "                pbar.update(1)\n",
    "                \n",
    "        self.sfs_results = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "        self.sfs_features = sfs.k_feature_idx_\n",
    "\n",
    "        # Evaluate model performance\n",
    "        X_train_subset = self.X_train_scaled[:, list(self.sfs_features)]\n",
    "        X_test_subset = self.X_test_scaled[:, list(self.sfs_features)]\n",
    "        model.fit(X_train_subset, self.y_train)\n",
    "        y_pred = model.predict(X_test_subset)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        print(f\"SFS Method - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "        return self.sfs_features , self.sfs_results\n",
    "\n",
    "    def embedded_method(self):\n",
    "        model = RandomForestClassifier(n_estimators=100)\n",
    "        model.fit(self.X_train_scaled, self.y_train)\n",
    "        importances = model.feature_importances_\n",
    "        self.feature_importances = dict(zip(self.X.columns, importances))\n",
    "        self.embedded_indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Plot feature importances\n",
    "        feature_names = np.array(self.X.columns)[self.embedded_indices]\n",
    "        sorted_importances = importances[self.embedded_indices]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(feature_names, sorted_importances, color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Feature Importances')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        X_train_subset = self.X_train_scaled[:, self.embedded_indices]\n",
    "        X_test_subset = self.X_test_scaled[:, self.embedded_indices]\n",
    "        accuracy, f1 = self.evaluate_model(model, X_train_subset, X_test_subset)\n",
    "        print(f\"Embedded Method - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        return self.embedded_indices\n",
    "\n",
    "    def plot_sfs(self):\n",
    "        if not hasattr(self, 'sfs_results'):\n",
    "            raise ValueError(\"SFS has not been run yet.\")\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(range(1, len(self.sfs_results) + 1), self.sfs_results['avg_score'], marker='o', color='b', label='F1 Score')\n",
    "        \n",
    "        # # Assuming accuracy was also recorded\n",
    "        # if 'avg_score' in self.sfs_results.columns:\n",
    "        #     plt.plot(range(1, len(self.sfs_results) + 1), self.sfs_results['avg_score'], marker='o', color='g', label='Accuracy')\n",
    "        \n",
    "        plt.title('Sequential Feature Selection (SFS) Performance')\n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def get_selected_features(self):\n",
    "        filter_features = self.filter_method()\n",
    "        rfe_features = self.wrapper_rfe()\n",
    "        sfs_features,sfs_results = self.wrapper_sfs()\n",
    "        embedded_features = self.embedded_method()\n",
    "        exhaustive_features = self.exhaustive_feature_selection(RandomForestClassifier(n_estimators=100))\n",
    "        \n",
    "        sfs_results.to_csv(\"sfs_results.csv\", index=False)\n",
    "        \n",
    "        return {\n",
    "            'Filter Method': filter_features,\n",
    "            'RFE Method': rfe_features,\n",
    "            'SFS Method': sfs_features,\n",
    "            'Exhaustive Method': exhaustive_features,\n",
    "            'Embedded Method': embedded_features[:25] # Top 25 features\n",
    "        }\n",
    "\n",
    "    def list_feature_importances(self):\n",
    "        if not hasattr(self, 'feature_importances'):\n",
    "            raise ValueError(\"Embedded method has not been run yet.\")\n",
    "        \n",
    "        return self.feature_importances\n",
    "    \n",
    " \n",
    "# ##Example Usage\n",
    "\n",
    "# selector = FeatureSelector(X, y)\n",
    "# selected_features = selector.get_selected_features()\n",
    "# print(\"Selected Features:\")\n",
    "# for method, features in selected_features.items():\n",
    "#     print(f\"{method}: {features}\")\n",
    "\n",
    "# # Print feature importances\n",
    "# feature_importances = selector.list_feature_importances()\n",
    "# print(\"\\nFeature Importances:\")\n",
    "# for feature, importance in feature_importances.items():\n",
    "#     print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# print(\"Feature Importances:\")\n",
    "# print(selector.list_feature_importances())\n",
    "\n",
    "# # Plot SFS performance\n",
    "# selector.wrapper_sfs()  # Run SFS to populate sfs_results\n",
    "# selector.plot_sfs()  # Plot SFS performance\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
